\chapter{基于自然语言处理模型的文件访问模式识别}
\section{自然语言处理发展概述}
自然语言处理（NLP）的定义可以简单概括对人类语言进行自动化、智能化分析以及学会人类表达的一系列计算机技术，是一门包含着计算机科学、时间序列分析以及语言学的交叉学科，这些学科既有区别又相互交叉。

1936年A.M.Turing发明了举世闻名的“图灵机”，使数学中的逻辑符号和真实世界之间建立了联系，为后来计算机的蓬勃发展提供了坚实的理论基础。20世纪50年代，在图灵机的计算模型的基础上，自动机理论被提出，是现代计算机科学发展的基础\cite{自然语言处理的历史与现状}。后来Kleene又在自动机理论模型之基础上提出了正则表达式和有限自动机。1956年，Chomsky提出了上下文无关的语法的理论，同年人工智能被发明后，被迅速应用到自然语言处理领域之中。上下文无关语法的提出使得该领域的研究分为了基于推理规则的符号派和基于概率论的随机派\cite{宋一凡2019自然语言处理的发展历史与现状}，在之后很多年里分别高速发展。70年代语音识别算法研制成功，隐马尔科夫模型（Hidden Markov Model，HMM）提出并得到了广泛应用\cite{自然语言处理的历史与现状}。

近年来，随着深度学习的飞速发展，自然语言处理领域也取得了诸多重要突破。RonanCollobert等\cite{Natural_language_processing_(almost)_from_scratch}于2011年的研究提出了一个简单的深度学习框架，在许多NLP经典任务中取得了前所未有的性能，如实体命名识别、语义标注和词性标注等。之后，研究人员提出了大量基于复杂深度学习的算法，用于解决有难度的NLP任务。2013年，Mikolv\cite{skipgram}提出了当前NLP领域最重要的模型之一Skip-gram，该模型以出色的性能表现将单词转化为高维向量，为后续如雨后春笋般涌现的自然语言处理模型奠定了基础。

本章后续内容以词向量方法、循环神经网络等主流自然语言处理模型为基础，探讨其在文件系统优化，尤其是数据迁移策略中“冷”、“热”文件分类问题的应用。

\section{基于词向量模型的文件名向量化}
\subsection{词向量概述}
\subsection{基于子词模型的文件名向量化}
\linkout{Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality}{1}
词向量方法（Word Embedding）是将单词表征为
词向量是将单词在向量空间中分布式表示的自然语言模型，该类模型可帮助学习算法通过对相似单词进行分组来在自然语言处理任务中实现更好的性能。{\color{orange}最早使用单词表示法的方法可以追溯到1986年Rumelhart，Hinton和Williams的工作}。此想法此后已成功应用于统计语言建模[1]。后续工作包括自动语音识别和机器翻译[14、7]以及各种NLP任务[2、20、15、3、18、19、9]的应用。
One-hot vector虽然自然，但是用处有限。比如，在互联网广告系统里，如果用户输入的query是“母亲节”，而有一个广告的关键词是“康乃馨”。虽然按照常理，我们知道这两个词之间是有联系的——母亲节通常应该送给母亲一束康乃馨；但是这两个词对应的one-hot vectors之间的距离度量，无论是欧氏距离还是余弦相似度(cosine similarity)，由于其向量正交，都认为这两个词毫无相关性。 得出这种与我们相悖的结论的根本原因是：每个词本身的信息量都太小。所以，仅仅给定两个词，不足以让我们准确判别它们是否相关。要想精确计算相关性，我们还需要更多的信息——从大量数据里通过机器学习方法归纳出来的知识
Mikolov等于2013年引入了Skip-gram模型，这是一种从大量非结构化文本数据中学习单词的高质量向量表示的有效方法。与大多数以前使用的用于学习单词向量的神经网络体系结构不同，Skip-gram模型的训练不涉及密集矩阵乘法，因此训练过程非常高效：经过优化的单机实现可以在一天中培训超过1000亿个单词。

{\color{red}More details about Skip-gram.}

\linkout{Enriching_Word_Vectors_with_Subword_Information}{1}
{\color{orange}通过为每个单词分配不同的向量，学习此类表示形式的流行模型会忽略单词的形态。 这是一个限制，特别是对于具有大量词汇和许多稀有单词的语言。 在本文中，我们提出了一种基于跳过图模型的新方法，其中每个单词都表示为一包字符n-gram。 向量表示与每个字符n-gram相关； 单词被表示为这些表示的总和。 我们的方法快速，可以快速在大型语料库上训练模型，并允许我们为未出现在训练数据中的单词计算单词表示。 我们在词的相似性和类比任务上用九种不同的语言评估词的表示形式。 通过与最近提出的形态词表示法进行比较，我们表明，我们的向量在这些任务上达到了最先进的性能。}

\section{基于循环神经网络的文件访问模式分析}
\subsection{循环神经网络及其在时间序列分析中的应用}
\subsection{采用GRNN（Gated Recurrent Neural Network）的文件访问模式分析模型}
\section{本章小结}